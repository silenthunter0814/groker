\chapter{空间曲线}

\section{向量微分和曲线弧长}



\section{切向量 $\mathbf{T}$、法向量 $\mathbf{N}$ 和副法向量 $\mathbf{B}$}

在微分几何中，Frenet-Serret 标架（$\mathbf{T}$, $\mathbf{N}$, $\mathbf{B}$）是描述曲线局部特征的核心。

当曲线按弧长参数化时，切向量的模长恒为 1，这简化了所有的导数关系。

1. 切向量 $\mathbf{T}$ 的推导

定义曲线为 $\mathbf{r}(s)$。切向量定义为位置对弧长的变化率：
$$\mathbf{T} = \frac{d\mathbf{r}}{ds}$$

由于 $s$ 是弧长，根据定义 $\|\mathbf{T}\| = 1$。

2. 主法向量 $\mathbf{N}$ 与曲率 $\kappa$

由于 $\mathbf{T}$ 是单位向量，其模长平方为常数：$\mathbf{T} \cdot \mathbf{T} = 1$。两边对 $s$ 求导：
$$\frac{d\mathbf{T}}{ds} \cdot \mathbf{T} + \mathbf{T} \cdot \frac{d\mathbf{T}}{ds} = 0 \implies 2\mathbf{T} \cdot \frac{d\mathbf{T}}{ds} = 0$$

这证明了导向量 $d\mathbf{T}/ds$ 始终垂直于 $\mathbf{T}$。

曲率 $\kappa$：定义为切线方向随弧长变化的速率，即 $\kappa = \|\frac{d\mathbf{T}}{ds}\|$。

主法向量 $\mathbf{N}$：定义为 $d\mathbf{T}/ds$ 方向上的单位向量：
$$\frac{d\mathbf{T}}{ds} = \kappa \mathbf{N}$$

3. 副法向量 $\mathbf{B}$ 与挠率 $\tau$

为了构成右手正交标架，我们定义副法向量为：
$$\mathbf{B} = \mathbf{T} \times \mathbf{N}$$

挠率 $\tau$ 的得出：

我们要考察 $\mathbf{B}$ 随弧长的变化。对 $\mathbf{B}$ 求导：
$$\frac{d\mathbf{B}}{ds} = \frac{d\mathbf{T}}{ds} \times \mathbf{N} + \mathbf{T} \times \frac{d\mathbf{N}}{ds}$$

由于 $d\mathbf{T}/ds = \kappa \mathbf{N}$，而 $\mathbf{N} \times \mathbf{N} = 0$，第一项消失：
$$\frac{d\mathbf{B}}{ds} = \mathbf{T} \times \frac{d\mathbf{N}}{ds}$$

这意味着 $d\mathbf{B}/ds$ 垂直于 $\mathbf{T}$。同时，由于 $\mathbf{B}$ 是单位向量，$d\mathbf{B}/ds$ 也垂直于 $\mathbf{B}$。既然它同时垂直于 $\mathbf{T}$ 和 $\mathbf{B}$，它必须在 $\mathbf{N}$ 的方向上。

因此，我们定义：
$$\frac{d\mathbf{B}}{ds} = -\tau \mathbf{N}$$

这里的 $\tau$ 称为挠率（Torsion），负号是几何学上的约定，表示当 $\tau > 0$ 时，曲线随 $s$ 增加向副法向量定义的右手螺旋方向扭转。

总结

$\mathbf{T}$ 描述前进方向。

$\mathbf{N}$ 描述向哪弯曲，曲率 $\kappa$ 是弯曲程度（偏离直线的程度）。

$\mathbf{B}$ 描述运动平面的法线，挠率 $\tau$ 是扭曲程度（偏离平面的程度）。对于平面曲线（如 $y=x^2$），$\mathbf{r}'''$ 依然在 $xy$ 平面，外积后与 $z$ 轴垂直，因此 $\tau$ 恒等于 0。

曲率 $\kappa$ 的求解公式

曲率 $\kappa$ 的基本定义是切向量对弧长的变化率的模：
$$\kappa = \left\| \frac{d\mathbf{T}}{ds} \right\|$$

由于我们通常使用的是参数 $t$（如时间），根据链式法则：
$$\frac{d\mathbf{T}}{dt} = \frac{d\mathbf{T}}{ds} \cdot \frac{ds}{dt} = \frac{d\mathbf{T}}{ds} \cdot \|\mathbf{r}'(t)\|$$

因此有：
$$\kappa = \frac{\|\mathbf{T}'(t)\|}{\|\mathbf{r}'(t)\|}$$

我们知道切向量 $\mathbf{T} = \frac{\mathbf{r}'}{\|\mathbf{r}'\|}$，所以速度向量可以写为：

$$\mathbf{r}' = \|\mathbf{r}'\| \mathbf{T}$$

对上式两边关于 $t$ 求导（使用乘法法则）：
$$\mathbf{r}'' = (\|\mathbf{r}'\|)' \mathbf{T} + \|\mathbf{r}'\| \mathbf{T}'$$

利用外积（叉乘）消项

现在我们将 $\mathbf{r}'$ 和 $\mathbf{r}''$ 做外积：
$$\mathbf{r}' \times \mathbf{r}'' = (\|\mathbf{r}'\| \mathbf{T}) \times ((\|\mathbf{r}'\|)' \mathbf{T} + \|\mathbf{r}'\| \mathbf{T}')$$

利用外积的分配律：
$$\mathbf{r}' \times \mathbf{r}'' = \|\mathbf{r}'\| (\|\mathbf{r}'\|)' (\mathbf{T} \times \mathbf{T}) + \|\mathbf{r}'\|^2 (\mathbf{T} \times \mathbf{T}')$$

因为任何向量与自身的外积为零 ($\mathbf{T} \times \mathbf{T} = 0$)，所以：
$$\mathbf{r}' \times \mathbf{r}'' = \|\mathbf{r}'\|^2 (\mathbf{T} \times \mathbf{T}')$$

取两边的模长：
$$\|\mathbf{r}' \times \mathbf{r}''\| = \|\mathbf{r}'\|^2 \cdot \|\mathbf{T} \times \mathbf{T}'\|$$

因为 $\mathbf{T}$ 是单位向量，且我们已知 $\mathbf{T}'$ 垂直于 $\mathbf{T}$（见前文推导），所以 $\|\mathbf{T} \times \mathbf{T}'\| = \|\mathbf{T}\| \|\mathbf{T}'\| \sin(90^\circ) = \|\mathbf{T}'\|$。代入上式：

$$\|\mathbf{r}' \times \mathbf{r}''\| = \|\mathbf{r}'\|^2 \|\mathbf{T}'\|$$

解出 $\|\mathbf{T}'\|$：
$$\|\mathbf{T}'\| = \frac{\|\mathbf{r}' \times \mathbf{r}''\|}{\|\mathbf{r}'\|^2}$$

最后，将这个结果代回最初的曲率定义：
$$\kappa = \frac{\|\mathbf{T}'\|}{\|\mathbf{r}'\|} = \frac{\|\mathbf{r}' \times \mathbf{r}''\|}{\|\mathbf{r}'\|^2 \cdot \|\mathbf{r}'\|} = \frac{\|\mathbf{r}' \times \mathbf{r}''\|}{\|\mathbf{r}'\|^3}$$

这个推导巧妙地利用了 $\mathbf{r}''$ 在 $\mathbf{T}$ 方向（切向加速度）和 $\mathbf{N}$ 方向（法向加速度）的分解。外积操作自动过滤掉了不改变方向的切向部分，只留下了反映“弯曲”的法向部分，从而直接提取出了曲率。




$\mathbf{T}$, $\mathbf{N}$, $\mathbf{B}$ 求解步骤：

第一步：切向量 $\mathbf{T}$ (Tangent)

对曲线 $\mathbf{r}(t)$ 求导并单位化：
$$\mathbf{T} = \frac{\mathbf{r}'(t)}{\|\mathbf{r}'(t)\|}$$

第二步：副法向量 $\mathbf{B}$ (Binormal)

在 3D 中，通常先求 $\mathbf{B}$ 反而更容易。利用加速度（二阶导）和速度的外积：
$$\mathbf{B} = \frac{\mathbf{r}'(t) \times \mathbf{r}''(t)}{\|\mathbf{r}'(t) \times \mathbf{r}''(t)\|}$$

原理： 速度和加速度构成的平面被称为密切平面，$\mathbf{B}$ 就是该平面的法向量。

第三步：主法向量 $\mathbf{N}$ (Normal)

有了 $\mathbf{T}$ 和 $\mathbf{B}$，$\mathbf{N}$ 可以通过叉乘直接得出，确保右手系：$$\mathbf{N} = \mathbf{B} \times \mathbf{T}$$

\begin{example} 抛物线 $y = x^2$ 的切向量 $\mathbf{T}$、法向量 $\mathbf{N}$ 和副法向量 $\mathbf{B}$。

    参数化方程为 $\mathbf{r}(x) = (x, x^2)$。
    
    为了方便计算空间中的 $B$ 向量，我们可以将其视为在 $z=0$ 平面上的三维曲线，即 $\mathbf{r}(x) = (x, x^2, 0)$。
    
    首先计算位置向量的一阶和二阶导数：
    
    $\mathbf{r}'(x) = (1, 2x, 0)$
    
    $\mathbf{r}''(x) = (0, 2, 0)$
    
    一阶导的模长：$\|\mathbf{r}'(x)\| = \sqrt{1 + (2x)^2} = \sqrt{1 + 4x^2}$

    切向量 $\mathbf{T}$ (Tangent)
    
    直接对一阶导进行单位化：
    $$\mathbf{T} = \frac{\mathbf{r}'(x)}{\|\mathbf{r}'(x)\|} = \left( \frac{1}{\sqrt{1+4x^2}}, \frac{2x}{\sqrt{1+4x^2}}, 0 \right)$$

    副法向量 $\mathbf{B}$ (Binormal)
    
    对于平面曲线，副法向量总是垂直于平面。我们通过叉乘计算：
    $$\mathbf{r}' \times \mathbf{r}'' = \begin{vmatrix} \mathbf{i} & \mathbf{j} & \mathbf{k} \\ 1 & 2x & 0 \\ 0 & 2 & 0 \end{vmatrix} = (0, 0, 2)$$

    单位化后得到：
    $$\mathbf{B} = (0, 0, 1)$$
    
    (这说明抛物线始终在 $xy$ 平面内弯曲)

    主法向量 $\mathbf{N}$ (Normal)
    
    利用 $\mathbf{N} = \mathbf{B} \times \mathbf{T}$（满足右手系且指向凹侧）：
    
    $$\mathbf{N} = \begin{vmatrix} \mathbf{i} & \mathbf{j} & \mathbf{k} \\ 0 & 0 & 1 \\ \frac{1}{\sqrt{1+4x^2}} & \frac{2x}{\sqrt{1+4x^2}} & 0 \end{vmatrix} = \left( -\frac{2x}{\sqrt{1+4x^2}}, \frac{1}{\sqrt{1+4x^2}}, 0 \right)$$

    计算曲率 $\kappa$ (Curvature)
    
    曲率公式为：$$\kappa = \frac{\|\mathbf{r}' \times \mathbf{r}''\|}{\|\mathbf{r}'\|^3}$$
    
    代入已知量：
    
    $\|\mathbf{r}' \times \mathbf{r}''\| = 2$
    
    $\|\mathbf{r}'\| = (1+4x^2)^{1/2}$

    得到：
    $$\kappa(x) = \frac{2}{(1 + 4x^2)^{3/2}}$$

    在顶点 $(0,0)$ 处： $\kappa = 2$。此时曲率最大，密切圆半径 $R = 1/\kappa = 0.5$。
    
    当 $x \to \infty$ 时： $\kappa \to 0$。这符合直觉，因为抛物线远端越来越趋于直线。

\end{example}


\section{拉格朗日乘子法}

拉格朗日乘子法是用来解决带约束的极值问题的经典方法。

为了直观推导，我们从最简单的二维情况入手：假设我们要找函数 $f(x, y)$ 的极值，但受到约束条件 $g(x, y) = c$ 的限制。

1. 几何直观：切线与梯度

想象你在登山，山的高度由 $f(x, y)$ 表示，而你只能沿着地面上的一条小路 $g(x, y) = c$ 行走。

等值线： 函数 $f$ 的等值线是 $f(x, y) = d$ 的曲线。

约束线： 约束条件 $g(x, y) = c$ 也是平面上的一条曲线。

当你沿着约束线行走时，只要约束线与 $f$ 的等值线相交，就说明你还在往更高或更低的地方走（即 $f$ 的值还在变化）。只有当约束线与 $f$ 的等值线相切时，你才达到了局部的最高点或最低点。

2. 梯度的物理意义

在相切的那一点，两条曲线的法向量（即梯度）必须在同一条直线上。

$f$ 的梯度 $\nabla f$ 指向函数增长最快的方向。

$g$ 的梯度 $\nabla g$ 垂直于约束曲线 $g(x, y) = c$。

既然在极值点处两条曲线相切，那么它们的梯度向量必须是共线（平行）的。用数学语言表达就是：$$\nabla f = \lambda \nabla g$$

这里的 $\lambda$ 就是拉格朗日乘子。它代表了两个梯度向量之间的比例关系。

3. 拉格朗日函数的构建

为了统一求解，我们将上述条件改写为：
$$\nabla f - \lambda \nabla g = 0$$

同时，我们不能忘记原始的约束条件：
$$g(x, y) - c = 0$$

为了同时满足这两个方程，拉格朗日天才地定义了一个新的函数，称为拉格朗日函数：

$$\mathcal{L}(x, y, \lambda) = f(x, y) - \lambda(g(x, y) - c)$$

拉格朗日乘子法的核心逻辑可以概括为：

极值必要条件：在约束边界上，目标函数的梯度与约束面的法向量平行。

升维简化：通过引入辅助变量 $\lambda$，把一个受限的 $n$ 维空间问题，变成了一个无约束的 $n+1$ 维空间求驻点的问题。

\begin{example} 假设矩形的两条边长分别为 $x$ 和 $y$。目标函数（面积）：$f(x, y) = xy$  （我们要使它最大）。约束条件（周长）：$2x + 2y = L$（$L$ 是常数），变形为 $g(x, y) = 2x + 2y - L = 0$。

    构建拉格朗日函数，引入乘子 $\lambda$，构造函数 $\mathcal{L}$：
    $$\mathcal{L}(x, y, \lambda) = xy - \lambda(2x + 2y - L)$$

    我们需要对 $x, y, \lambda$ 分别求偏导，找到平稳点：
    
    $\frac{\partial \mathcal{L}}{\partial x} = y - 2\lambda = 0 \quad \Rightarrow \quad y = 2\lambda$
    
    $\frac{\partial \mathcal{L}}{\partial y} = x - 2\lambda = 0 \quad \Rightarrow \quad x = 2\lambda$
    
    $\frac{\partial \mathcal{L}}{\partial \lambda} = -(2x + 2y - L) = 0 \quad \Rightarrow \quad 2x + 2y = L$

    由前两个方程可知：
    $$x = y = 2\lambda$$

    这意味着在极值点，矩形的邻边相等。
    
    将 $x = y$ 代入第三个方程（约束条件）：
    $$2x + 2x = L$$
    $$4x = L \quad \Rightarrow \quad x = \frac{L}{4}$$

    所以：
    $$x = y = \frac{L}{4}$$
    $$S_{max} = \frac{L}{4} \times \frac{L}{4} = \frac{L^2}{16}$$
\end{example}

在这个例子中：

$\nabla f$ (目标梯度)：是 $(y, x)$，它指向面积增加的方向。

$\nabla g$ (约束梯度)：是 $(2, 2)$，它是周长线的法向量，垂直于直线 $2x+2y=L$。

核心逻辑：当 $y - 2\lambda = 0$ 且 $x - 2\lambda = 0$ 时，即 $(y, x) = \lambda(2, 2)$。这说明面积函数的等值线（双曲线）正好与周长约束线（直线）相切，此时切点坐标就是 $(L/4, L/4)$。




\section{机器学习中的常用函数及其导函数}

在机器学习中，激活函数（Activation Functions）和损失函数（Loss Functions）是模型学习的核心。了解它们的导函数（梯度）至关重要，因为反向传播算法正是利用导数来更新权重。

1. 激活函数 (Activation Functions)

激活函数为神经网络引入非线性，使其能够拟合复杂的函数。

Sigmoid 函数

Sigmoid 常用于二分类任务的输出层，将输入映射到 $(0, 1)$ 之间。

函数表达式： $f(x) = \frac{1}{1 + e^{-x}}$

导函数： $f'(x) = f(x)(1 - f(x))$


我们将使用导函数的商法则 (Quotient Rule) 或 链式法则 (Chain Rule) 来完成推导

基础导数公式：

$\frac{d}{dx}(e^{u}) = e^{u} \cdot \frac{du}{dx}$

$\frac{d}{dx}(x^n) = nx^{n-1}$

Sigmoid 函数定义：
$$f(x) = \frac{1}{1 + e^{-x}} = (1 + e^{-x})^{-1}$$

我们可以将 $f(x)$ 看作一个复合函数，设 $u = 1 + e^{-x}$，则 $f(x) = u^{-1}$。

对外部函数求导：
$$\frac{df}{du} = -1 \cdot u^{-2} = -\frac{1}{(1 + e^{-x})^2}$$

对内部函数 $u$ 求导：
$$\frac{du}{dx} = \frac{d}{dx}(1 + e^{-x}) = 0 + e^{-x} \cdot (-1) = -e^{-x}$$

应用链式法则

将上述两部分相乘：
$$f'(x) = \frac{df}{du} \cdot \frac{du}{dx} = \left( -\frac{1}{(1 + e^{-x})^2} \right) \cdot (-e^{-x})$$

$$f'(x) = \frac{e^{-x}}{(1 + e^{-x})^2}$$

转化为自身表示：
\begin{align*}
    f'(x) &= \frac{1}{1 + e^{-x}} \cdot \frac{e^{-x}}{1 + e^{-x}} \\
    &= \frac{1}{1 + e^{-x}} \cdot \frac{(1 + e^{-x}) - 1}{1 + e^{-x}} \\
    &= \frac{1}{1 + e^{-x}} \cdot \left( \frac{1 + e^{-x}}{1 + e^{-x}} - \frac{1}{1 + e^{-x}} \right) \\
    &= \frac{1}{1 + e^{-x}} \cdot \left( 1 - \frac{1}{1 + e^{-x}} \right)
\end{align*}

代回原函数： 因为 $f(x) = \frac{1}{1 + e^{-x}}$
$$f'(x) = f(x)(1 - f(x))$$

Sigmoid 函数的优缺点：

计算优势： 在神经网络反向传播时，我们已经计算过了前向传播的激活值 $f(x)$。有了这个公式，求导就不再需要重新计算复杂的指数幂，只需一次减法和一次乘法，极大地提升了效率。

梯度消失： 观察公式 $f(x)(1-f(x))$。当 $x$ 很大时，$f(x) \approx 1$，导数趋于 $1(1-1)=0$；当 $x$ 很小时，$f(x) \approx 0$，导数趋于 $0(1-0)=0$。这就是为什么神经元在这些区域会“饱和”，导致权重无法更新。


ReLU 函数

ReLU 函数（Rectified Linear Unit，修正线性单元）是目前深度学习中最受欢迎、应用最广泛的激活函数。它的结构虽然极其简单，但却解决了深度神经网络训练中的许多难题。

ReLU 函数本质上是一个分段线性函数，它会将所有负值归零，而保留所有正值。

数学表达式：
$$f(x) = \max(0, x)$$

分段形式：
$$f(x) = \begin{cases} x, & \text{if } x \ge 0 \\ 0, & \text{if } x < 0 \end{cases}$$

由于 ReLU 在 $x > 0$ 时是线性函数，在 $x < 0$ 时是常数函数，其导数非常容易计算。

导数表达式：
$$f'(x) = \begin{cases} 1, & \text{if } x > 0 \\ 0, & \text{if } x < 0 \end{cases}$$

关于 $x=0$ 点： 在数学上，ReLU 在 $0$ 点是不可导的（左导数为 0，右导数为 1）。但在机器学习的实际实现中，通常人为定义 $f'(0) = 0$ 或 $1$。

在 ReLU 出现之前，Sigmoid 是主流，但 ReLU 凭借以下优势改变了局面：

缓解梯度消失： 在 $x > 0$ 的区域，ReLU 的导数恒为 $1$。这意味着在反向传播时，梯度可以无损地传递，不会像 Sigmoid 那样因为多次相乘而迅速趋近于 0。这使得训练深层神经网络变得更加容易。

计算速度极快： Sigmoid 涉及指数运算（$e^{-x}$），计算量大；而 ReLU 只需要一个阈值判断（if x > 0），在大规模模型中能显著节省计算资源。

稀疏激活： 在同一个批次的数据中，通常会有大量的神经元输出为 0。这种“稀疏性”使得模型更具代表性，且能起到一定的正则化作用，防止过拟合。

2. 均方误差 (MSE)

均方误差（Mean Squared Error, MSE）是回归任务中最常用的损失函数。它衡量的是模型预测值与真实值之间差异的平方的平均值。

假设我们有 $n$ 个样本，对于每个样本 $i$，真实值为 $y_i$，模型预测值为 $\hat{y}_i$。

单个样本的误差： $e_i = (y_i - \hat{y}_i)^2$

全样本的 MSE 表达式：
$$L = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

注意： 在机器学习推导（如梯度下降）中，为了求导后能抵消掉平方项产生的系数 $2$，通常会在公式前加上 $\frac{1}{2}$，写成：
$$L = \frac{1}{2n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

导函数推导

我们通常需要对预测值 $\hat{y}_i$ 求导，以确定如何调整模型参数来减小损失。

步骤 1：应用链式法则设 $u_i = y_i - \hat{y}_i$，则 $L = \frac{1}{2n} \sum u_i^2$。根据链式法则：
$$\frac{\partial L}{\partial \hat{y}_i} = \frac{\partial L}{\partial u_i} \cdot \frac{\partial u_i}{\partial \hat{y}_i}$$

步骤 2：分别求导

对外部平方项求导：$\frac{\partial L}{\partial u_i} = \frac{1}{2n} \cdot 2u_i = \frac{1}{n} u_i$

对内部差值项求导：$\frac{\partial u_i}{\partial \hat{y}_i} = \frac{\partial (y_i - \hat{y}_i)}{\partial \hat{y}_i} = -1$

步骤 3：组合结果
$$\frac{\partial L}{\partial \hat{y}_i} = \frac{1}{n} (y_i - \hat{y}_i) \cdot (-1) = \frac{1}{n} (\hat{y}_i - y_i)$$

对于第 $i$ 个样本，MSE 对预测值的梯度就是 预测值与真实值之差（缩放 $1/n$ 倍）。

MSE 的特性

惩罚离群点： 由于存在平方项，当预测值与真实值差距较大时，误差会呈二次方增长。这意味着 MSE 对异常值（Outliers）非常敏感。

凸性（Convexity）： MSE 是一个凸函数。在线性回归中，这意味着它只有一个全局最小值，使用梯度下降法很容易找到最优解。

物理意义： MSE 对应于高斯分布下的最大似然估计（MLE）。


3. Softmax 函数

Softmax 函数是机器学习中处理多分类问题的核心函数。它通常位于神经网络的最后一层，负责将原始的得分（Logits）转换为概率分布。

函数定义

假设我们有一个向量 $\mathbf{z}$，包含 $K$ 个类别的原始得分 $z_1, z_2, \dots, z_K$。Softmax 函数将其中第 $i$ 个分量映射为一个 $(0, 1)$ 之间的概率值 $a_i$。

数学表达式：
$$a_i = \text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

核心特性：

归一化： 所有输出概率之和恒等于 1，即 $\sum_{i=1}^{K} a_i = 1$。

放大差异： 通过指数函数 $e^z$，Softmax 会“拉开”得分之间的差距，使得得分最高的类别在概率分布中占据主导地位。

导函数推导

Softmax 的求导比 Sigmoid 复杂一些，因为它是一个“多对多”的函数——输出向量中的每一个元素 $a_i$ 都依赖于输入向量中的每一个元素 $z_j$。

我们需要计算偏导数 $\frac{\partial a_i}{\partial z_j}$。根据 $i$ 是否等于 $j$，分为两种情况：

准备工作：商法则

回顾导数商法则：$\left(\frac{u}{v}\right)' = \frac{u'v - uv'}{v^2}$。在 Softmax 中：

$u = e^{z_i}$

$v = \sum_{k=1}^{K} e^{z_k}$

情况 1：$i = j$（对自身得分求导）

此时 $u' = \frac{\partial e^{z_i}}{\partial z_i} = e^{z_i}$，$v' = \frac{\partial (\sum e^{z_k})}{\partial z_i} = e^{z_i}$。

$$\frac{\partial a_i}{\partial z_i} = \frac{e^{z_i} \cdot \sum - e^{z_i} \cdot e^{z_i}}{(\sum)^2}$$

$$\frac{\partial a_i}{\partial z_i} = \frac{e^{z_i}}{\sum} \cdot \frac{\sum - e^{z_i}}{\sum} = a_i(1 - a_i)$$

情况 2：$i \neq j$（对其他得分求导）

此时 $u = e^{z_i}$ 对 $z_j$ 求导为 0（因为 $i \neq j$），$v' = \frac{\partial (\sum e^{z_k})}{\partial z_j} = e^{z_j}$。

$$\frac{\partial a_i}{\partial z_j} = \frac{0 \cdot \sum - e^{z_i} \cdot e^{z_j}}{(\sum)^2}$$

$$\frac{\partial a_i}{\partial z_j} = - \frac{e^{z_i}}{\sum} \cdot \frac{e^{z_j}}{\sum} = -a_i a_j$$

综合结论：
$$\frac{\partial a_i}{\partial z_j} = \begin{cases} a_i(1 - a_j), & \text{if } i = j \\ -a_i a_j, & \text{if } i \neq j \end{cases}$$

与交叉熵结合

在深度学习框架中，Softmax 通常与交叉熵损失 (Cross-Entropy Loss) 配合使用。有趣的是，当它们结合在一起时，最终的梯度形式会变得异常简洁。

设 $y_i$ 为真实标签（One-hot 编码），$a_i$ 为 Softmax 输出，总损失为 $L = -\sum y_i \ln a_i$。经过链式法则推导，损失 $L$ 对输入 $z_j$ 的梯度简单到令人惊讶：

$$\frac{\partial L}{\partial z_j} = a_j - y_j$$

直观理解： 梯度就是“预测概率与真实标签的差值”。如果预测概率比真实值大，梯度为正，减小该权重；反之亦然。这种完美的抵消特性正是 Softmax + 交叉熵成为分类任务标配的原因。

在计算 Softmax 时，如果 $z_i$ 非常大，$e^{z_i}$ 会导致数值溢出。在计算前减去向量中的最大值，即 $e^{z_i - \max(\mathbf{z})}$。这不会改变输出结果，但能保证计算安全。

4. 交叉熵损失 (Cross-Entropy Loss)

交叉熵损失是分类问题中最核心的损失函数。它源于信息论，用于衡量两个概率分布之间的“距离”。

在机器学习中，我们希望模型输出的概率分布 $P$ 尽可能接近数据的真实概率分布 $Q$。

函数定义

二分类 (Binary Cross-Entropy, BCE)

当我们只有两个类别（如：猫或狗）时，模型输出一个值 $\hat{y} \in (0, 1)$。

数学表达式：
$$L = -[y \ln(\hat{y}) + (1-y) \ln(1-\hat{y})]$$

其中 $y$ 为真实标签（0 或 1），$\hat{y}$ 为模型预测概率。

多分类 (Categorical Cross-Entropy)

当有 $K$ 个类别时，使用 Softmax 输出概率分布 $\mathbf{\hat{y}}$。

数学表达式：
$$L = -\sum_{i=1}^{K} y_i \ln(\hat{y}_i)$$

其中 $y_i$ 是真实标签的 One-hot 编码（正确类为 1，其余为 0）。

二分类导函数推导

在神经网络中，$\hat{y}$ 通常由 Sigmoid 函数产生，即 $\hat{y} = \sigma(z) = \frac{1}{1+e^{-z}}$。我们需要求对 $z$ 的梯度。

步骤 1：利用链式法则$$\frac{\partial L}{\partial z} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z}$$

步骤 2：对损失函数求导 $\frac{\partial L}{\partial \hat{y}}$

对 $L = -y \ln\hat{y} - (1-y)\ln(1-\hat{y})$ 求偏导：
$$\frac{\partial L}{\partial \hat{y}} = -\frac{y}{\hat{y}} + \frac{1-y}{1-\hat{y}} = \frac{\hat{y}-y}{\hat{y}(1-\hat{y})}$$

步骤 3：结合 Sigmoid 的导数

前面推导过 $\frac{\partial \hat{y}}{\partial z} = \hat{y}(1-\hat{y})$。将两者相乘：

$$\frac{\partial L}{\partial z} = \frac{\hat{y}-y}{\hat{y}(1-\hat{y})} \cdot \hat{y}(1-\hat{y})$$$$\frac{\partial L}{\partial z} = \hat{y} - y$$

梯度同样非常简洁，即 预测概率与真实标签的差值。

多分类导函数推导

在多分类任务中，Softmax 激活函数与**交叉熵损失（Cross-Entropy Loss）**的结合是标准配置。它们的组合推导过程展示了数学上的巧妙抵消。

Softmax 输出： $a_i = \frac{e^{z_i}}{\sum_{k=1}^K e^{z_k}}$

多分类交叉熵损失： $L = -\sum_{i=1}^K y_i \ln a_i$

其中 $y_i$ 是标签的 One-hot 编码。对于一个样本，只有一个 $y_{true}=1$，其余均为 0。

Softmax 自身的偏导（前文已证）：
$$\frac{\partial a_i}{\partial z_j} = \begin{cases} a_i(1 - a_i) & i = j \\ -a_i a_j & i \neq j \end{cases}$$

根据链式法则，由于 $L$ 是关于所有 $a_i$ 的函数，而每个 $a_i$ 又是关于 $z_j$ 的函数：

$$\frac{\partial L}{\partial z_j} = \sum_{i=1}^K \frac{\partial L}{\partial a_i} \cdot \frac{\partial a_i}{\partial z_j}$$

对 $L = -\sum y_i \ln a_i$ 求导：
$$\frac{\partial L}{\partial a_i} = -\frac{y_i}{a_i}$$

由于 $\frac{\partial a_i}{\partial z_j}$ 在 $i=j$ 和 $i \neq j$ 时表现不同，我们将求和式拆分为两部分：
$$\frac{\partial L}{\partial z_j} = \left( \frac{\partial L}{\partial a_j} \cdot \frac{\partial a_j}{\partial z_j} \right) + \sum_{i \neq j}^K \left( \frac{\partial L}{\partial a_i} \cdot \frac{\partial a_i}{\partial z_j} \right)$$

代入具体导数
$$\frac{\partial L}{\partial z_j} = \left( -\frac{y_j}{a_j} \cdot a_j(1 - a_j) \right) + \sum_{i \neq j}^K \left( -\frac{y_i}{a_i} \cdot (-a_i a_j) \right)$$

左边项简化： $-y_j(1 - a_j) = -y_j + y_j a_j$

右边项简化： $\sum_{i \neq j}^K (y_i a_j) = a_j \sum_{i \neq j}^K y_i$

合并与最终简化
$$\frac{\partial L}{\partial z_j} = -y_j + y_j a_j + a_j \sum_{i \neq j}^K y_i$$

$$\frac{\partial L}{\partial z_j} = -y_j + a_j \left( y_j + \sum_{i \neq j}^K y_i \right)$$

因为 $y$ 是 One-hot 编码，所有概率之和 $\sum_{i=1}^K y_i = 1$，所以括号内的部分等于 1：

$$\frac{\partial L}{\partial z_j} = a_j - y_j$$

这个结果非常优雅：梯度 = 预测值 - 真实值。

如果预测很准： 比如类别 $j$ 是正确的 ($y_j=1$)，模型预测 $a_j=0.99$，那么梯度 $0.99 - 1 = -0.01$，权重只会有极小的修正。

如果预测很差： 比如类别 $j$ 是正确的 ($y_j=1$)，但模型预测 $a_j=0.1$，那么梯度 $0.1 - 1 = -0.9$，巨大的梯度会强制模型大幅度调整权重。

在编写深度学习框架（如 PyTorch 的 CrossEntropyLoss）时，通常会将 Softmax 和 CrossEntropy 合并计算。这不仅是为了求导公式的简洁，更是为了数值稳定性。直接计算 $a_j - y_j$ 避免了中间步骤中可能出现的 log(0) 或极大指数幂溢出的问题。




\section{通量和散度}

通量的物理直觉是：衡量矢量场 $\mathbf{F}$ 在单位时间内穿过某个微小表面的“净流量”。

在直角坐标系下，我们推导矢量场 $\mathbf{F} = (F_1, F_2, F_3)$ 流出一个微元体（体积 $dV = dx dy dz$）的净通量。

1. 建立微元模型

想象一个中心位于 $(x, y, z)$ 的微小长方体，六个面分别垂直于坐标轴。我们以穿过垂直于 $x$ 轴的两个表面的通量为例进行推导。

右侧面（位于 $x + \frac{dx}{2}$）：法向量指向 $+x$ 方向。该面中心处的场分量 $F_1$ 约为：
$$F_1(x + \frac{dx}{2}, y, z) \approx F_1 + \frac{\partial F_1}{\partial x} \frac{dx}{2}$$

穿出通量：$\Phi_{right} = (F_1 + \frac{\partial F_1}{\partial x} \frac{dx}{2}) \cdot dy dz$

左侧面（位于 $x - \frac{dx}{2}$）：法向量指向 $-x$ 方向（流出体积的方向）。该面中心处的场分量 $F_1$ 约为：
$$F_1(x - \frac{dx}{2}, y, z) \approx F_1 - \frac{\partial F_1}{\partial x} \frac{dx}{2}$$

左侧面（位于 $x - \frac{dx}{2}$）：法向量指向 $-x$ 方向（流出体积的方向）。该面中心处的场分量 $F_1$ 约为：
$$F_1(x - \frac{dx}{2}, y, z) \approx F_1 - \frac{\partial F_1}{\partial x} \frac{dx}{2}$$

穿出通量（由于法向量向左，需乘面积向量 $-dy dz \mathbf{i}$）：$\Phi_{left} = -(F_1 - \frac{\partial F_1}{\partial x} \frac{dx}{2}) \cdot dy dz$

$x$ 方向的净通量（两面之和）：

$$d\Phi_x = \Phi_{right} + \Phi_{left} = \left( \frac{\partial F_1}{\partial x} dx \right) dy dz = \frac{\partial F_1}{\partial x} dV$$

3. 汇总三个方向

同理，我们可以得到穿过另外两对面的净通量：

$y$ 方向净通量： $d\Phi_y = \frac{\partial F_2}{\partial y} dy (dx dz) = \frac{\partial F_2}{\partial y} dV$

$z$ 方向净通量： $d\Phi_z = \frac{\partial F_3}{\partial z} dz (dx dy) = \frac{\partial F_3}{\partial z} dV$

总净通量（流出该微元体的总和）：
$$d\Phi = d\Phi_x + d\Phi_y + d\Phi_z = \left( \frac{\partial F_1}{\partial x} + \frac{\partial F_2}{\partial y} + \frac{\partial F_3}{\partial z} \right) dx dy dz$$

4. 结论：从通量到散度

通过上面的推导，我们发现了一个极其重要的物理量：

特征量：括号内的 $\left( \frac{\partial F_1}{\partial x} + \frac{\partial F_2}{\partial y} + \frac{\partial F_3}{\partial z} \right)$ 描述了该点处通量的源性。

散度的诞生：如果我们定义单位体积的净通量为该点的“散度”，那么：

$$\text{div } \mathbf{F} = \frac{d\Phi}{dV} = \frac{\partial F_1}{\partial x} + \frac{\partial F_2}{\partial y} + \frac{\partial F_3}{\partial z}$$

算子表示：这恰好对应 $\nabla$ 算子与 $\mathbf{F}$ 的点乘：$\nabla \cdot \mathbf{F}$。






\section{环量和旋度}

1. 物理直觉：环量密度

如果一个场在某处有旋转趋势，那么你沿着一个极小的闭合回路 $L$ 走一圈，矢量场沿路径的累积效应（环量） $\oint_L \mathbf{F} \cdot d\mathbf{l}$ 就不应该为零。

我们要寻找一个量，用来描述 $\mathbf{F}$ 在某点附近的旋转强度。这个量不应该依赖于我们取的回路大小，因此我们要计算的是单位面积的环量极限。

1. 考察 $xy$ 平面内的旋转 (绕 $z$ 轴)

我们在 $xy$ 平面上取一个微小矩形，中心为 $(x, y, z)$，边长为 $dx, dy$。我们计算沿边界逆时针走一圈的环量 $\Gamma_z = \oint \mathbf{F} \cdot d\mathbf{l}$。

底边 (1)： 路径 $dx$，位于 $y - \frac{dy}{2}$。$F_1$ 的贡献：
$$\left( F_1(x, y - \frac{dy}{2}, z) \right) dx \approx (F_1 - \frac{\partial F_1}{\partial y}\frac{dy}{2}) dx$$

顶边 (2)： 路径 $-dx$ (向左)，位于 $y + \frac{dy}{2}$。$F_1$ 的贡献：
$$-\left( F_1(x, y + \frac{dy}{2}, z) \right) dx \approx -(F_1 + \frac{\partial F_1}{\partial y}\frac{dy}{2}) dx$$

右边 (3)： 路径 $dy$ (向上)，位于 $x + \frac{dx}{2}$。$F_2$ 的贡献：
$$\left( F_2(x + \frac{dx}{2}, y, z) \right) dy \approx (F_2 + \frac{\partial F_2}{\partial x}\frac{dx}{2}) dy$$

左边 (4)： 路径 $-dy$ (向下)，位于 $x - \frac{dx}{2}$。$F_2$ 的贡献：
$$-\left( F_2(x - \frac{dx}{2}, y, z) \right) dy \approx -(F_2 - \frac{\partial F_2}{\partial x}\frac{dx}{2}) dy$$

累加总环量 $\Gamma_z$：
$$\Gamma_z = \underbrace{\left[ (F_2 + \frac{\partial F_2}{\partial x}\frac{dx}{2}) - (F_2 - \frac{\partial F_2}{\partial x}\frac{dx}{2}) \right] dy}_{\text{垂直边贡献}} + \underbrace{\left[ (F_1 - \frac{\partial F_1}{\partial y}\frac{dy}{2}) - (F_1 + \frac{\partial F_1}{\partial y}\frac{dy}{2}) \right] dx}_{\text{水平边贡献}}$$

简化后得到：
$$\Gamma_z = \left( \frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y} \right) dx dy$$

由此定义绕 $z$ 轴的旋转强度（单位面积环量）：
$$\omega_z = \frac{\Gamma_z}{dx dy} = \frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y}$$

2. 推广到其他两个平面

利用同样的逻辑（轮换对称性），我们可以求出场在其他两个正交平面上的旋转强度：

在 $yz$ 平面内 (绕 $x$ 轴)：考察 $F_2$ 和 $F_3$ 随 $y$ 和 $z$ 的变化：
$$\omega_x = \frac{\partial F_3}{\partial y} - \frac{\partial F_2}{\partial z}$$

在 $zx$ 平面内 (绕 $y$ 轴)：考察 $F_3$ 和 $F_1$ 随 $z$ 和 $x$ 的变化：
$$\omega_y = \frac{\partial F_1}{\partial z} - \frac{\partial F_3}{\partial x}$$

3. 合成旋度矢量

至此，我们发现这三个量 $(\omega_x, \omega_y, \omega_z)$ 完整地描述了矢量场在这一点三个维度的旋转特性。我们将这三个分量组合成一个新的矢量，并命名为 $\text{curl } \mathbf{F}$：

$$\text{curl } \mathbf{F} = \left( \frac{\partial F_3}{\partial y} - \frac{\partial F_2}{\partial z} \right) \mathbf{i} + \left( \frac{\partial F_1}{\partial z} - \frac{\partial F_3}{\partial x} \right) \mathbf{j} + \left( \frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y} \right) \mathbf{k}$$

4. 符号化的最终形式

为了方便记忆这种复杂的偏微分组合，我们引入算子 2$\nabla = (\frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z})$。观察3发现，上述结果恰好等于 $\nabla$ 与 $\mathbf{F}$ 的叉乘结果：

$$\nabla \times \mathbf{F} = \begin{vmatrix} \mathbf{i} & \mathbf{j} & \mathbf{k} \\ \frac{\partial}{\partial x} & \frac{\partial}{\partial y} & \frac{\partial}{\partial z} \\ F_1 & F_2 & F_3 \end{vmatrix}$$

总结比较

概念	微元基础	物理意义	最终形成的算子

环量微元

闭合线积分 $\oint \mathbf{F} \cdot d\mathbf{l}$描述局部的旋转旋度 $\nabla \times \mathbf{F}$

通量微元

闭合面积分 $\oint \mathbf{F} \cdot d\mathbf{S}$ 描述局部的发散/汇聚散度 $\nabla \cdot \mathbf{F}$


\section{格林定理}

格林定理（Green's Theorem）是向量分析中一个非常优美的结论，它将闭合曲线上的线积分与该曲线所围区域上的面积分联系了起来。

实际上，格林定理可以看作是旋度微元原理在二维平面上的宏观累加。

1. 物理直觉：小漩涡合成大漩涡

想象一个平面区域 $D$，被一条闭合曲线 $C$ 包围。我们将区域 $D$ 划分为无数个极其微小的矩形元（即我们之前推导过的“环量微元”）。

内部抵消：对于两个相邻的微元，它们相交的边会被经过两次，但方向相反。因此，沿这些内部公共边的线积分会全部相互抵消。

边界残留：唯一没有被抵消掉的，只有最外层紧贴曲线 $C$ 的那些微元边。

结论：所有微元环量的总和，等于绕最外圈大边界的环量。

2. 数学推导过程

设平面矢量场为 $\mathbf{F} = P(x, y)\mathbf{i} + Q(x, y)\mathbf{j}$。我们要证明：

$$\oint_C (P dx + Q dy) = \iint_D \left( \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} \right) dA$$

第一步：处理 $Q$ 分量的部分

我们先考察面积分中的 $\iint_D \frac{\partial Q}{\partial x} dA$。假设区域 $D$ 是一个简单的 $x$-型区域（左边界为 $x=g_1(y)$，右边界为 $x=g_2(y)$）：

$$\iint_D \frac{\partial Q}{\partial x} dx dy = \int_{c}^{d} \left[ \int_{g_1(y)}^{g_2(y)} \frac{\partial Q}{\partial x} dx \right] dy$$

据微积分基本定理，内部积分结果为：
$$\int_{g_1(y)}^{g_2(y)} \frac{\partial Q}{\partial x} dx = Q(g_2(y), y) - Q(g_1(y), y)$$

代回原式：
$$\iint_D \frac{\partial Q}{\partial x} dA = \int_{c}^{d} Q(g_2(y), y) dy - \int_{c}^{d} Q(g_1(y), y) dy$$

这两个积分正好对应曲线 $C$ 的右半部分和左半部分对 $dy$ 的积分。合并后即为：
$$\iint_D \frac{\partial Q}{\partial x} dA = \oint_C Q dy$$

第二步：处理 $P$ 分量的部分

同理，假设区域 $D$ 是一个 $y$-型区域（下边界 $y=f_1(x)$，上边界 $y=f_2(x)$），计算 $\iint_D \frac{\partial P}{\partial y} dA$：

$$\iint_D \frac{\partial P}{\partial y} dy dx = \int_{a}^{b} [P(x, f_2(x)) - P(x, f_1(x))] dx$$

注意，当我们沿着曲线 $C$ 逆时针走时，上边界是从右向左走的（$dx$ 为负），下边界是从左向右走的（$dx$ 为正）。因此：

$$\oint_C P dx = \int_{a}^{b} P(x, f_1(x)) dx + \int_{b}^{a} P(x, f_2(x)) dx = -\iint_D \frac{\partial P}{\partial y} dA$$

第三步：组合结果

将上述两部分相加：
$$\oint_C P dx + \oint_C Q dy = \iint_D \frac{\partial Q}{\partial x} dA - \iint_D \frac{\partial P}{\partial y} dA$$

合并后得到格林定理的标准形式：
$$\oint_C (P dx + Q dy) = \iint_D \left( \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} \right) dx dy$$

右边的被积函数：$\left( \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} \right)$。这正是我们之前推导的旋度在 $z$ 方向的分量（平面旋转强度）吗。

所以格林定理的本质就是：“区域内所有微小旋转的总和 = 边界上的总环流”。



